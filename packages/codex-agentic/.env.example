# Codex-Agentic Provider Configuration
# Copy this file to .env and fill in your API keys

# =============================================================================
# ANTHROPIC (Claude)
# =============================================================================
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
ANTHROPIC_MODEL=claude-sonnet-4-20250514
# ANTHROPIC_MAX_TOKENS=4096
# ANTHROPIC_TEMPERATURE=0.7
# ANTHROPIC_TIMEOUT=60000

# =============================================================================
# OPENAI (GPT)
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o
# OPENAI_MAX_TOKENS=4096
# OPENAI_TEMPERATURE=0.7
# OPENAI_TIMEOUT=60000

# =============================================================================
# GOOGLE GEMINI
# =============================================================================
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GEMINI_MODEL=gemini-1.5-pro
# GEMINI_MAX_TOKENS=4096
# GEMINI_TEMPERATURE=0.7
# GEMINI_TIMEOUT=60000

# =============================================================================
# DEEPSEEK
# =============================================================================
# Get your API key from: https://platform.deepseek.com/
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
DEEPSEEK_MODEL=deepseek-chat
# DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
# DEEPSEEK_MAX_TOKENS=4096
# DEEPSEEK_TEMPERATURE=0.7
# DEEPSEEK_TIMEOUT=60000

# =============================================================================
# LOCAL LLM (Ollama/LMStudio)
# =============================================================================
# No API key required - just make sure Ollama/LMStudio is running
# Install Ollama: https://ollama.ai/
# LOCAL_LLM_URL=http://localhost:11434
# LOCAL_LLM_MODEL=llama3.1:8b-instruct-q4_K_M
# LOCAL_LLM_MAX_TOKENS=4096
# LOCAL_LLM_TEMPERATURE=0.7
# LOCAL_LLM_TIMEOUT=120000

# =============================================================================
# GLOBAL SETTINGS
# =============================================================================
# Default provider to use when not specified
DEFAULT_LLM_PROVIDER=anthropic

# Fallback providers (comma-separated) if default fails
FALLBACK_LLM_PROVIDERS=openai,gemini,deepseek

# Retry settings
LLM_RETRY_ATTEMPTS=3
LLM_RETRY_DELAY=1000

# System prompt override for Codex deliberations
# CODEX_SYSTEM="Follow Virtue Accords. Prioritize integrity, privacy, safety."

# =============================================================================
# MEMORY & LEARNING (Phase 2)
# =============================================================================
# Enable/disable memory persistence for agent learning
CODEX_USE_MEMORY=true

# Directory for memory storage (file-based)
MEMORY_STORAGE_DIR=./.codex-memory

# Memory retrieval settings
MEMORY_MAX_CONTEXT_ENTRIES=5
MEMORY_SIMILARITY_THRESHOLD=0.1

# Auto-flush interval (ms) for memory persistence
MEMORY_FLUSH_INTERVAL=30000
